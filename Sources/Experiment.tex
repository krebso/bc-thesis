\chapter{Benchmark}

This chapter consists of three sections. First, we tackle the current problem of high computational resources utilization by occlusion-based saliency -- by measuring time and GPU memory it takes for a method to compute explanation. In the second part, we establishing quantitative benchmark, choosing suitable metrics alongside brief reasoning and evidence behind our choice. We compare methods introduced in Section N.M against Occlusion. We combine qualitative metrics capturing desired properties of explainability methods, alongside improved and refined metrics from original paper by Gallo et al [6]. In the last part, we conduct a human evaluation, where we present result of explainability methods to domain expert.

\section{What makes a good explanation?}

\emph{Unfortunately, “explainability” is a nebulous and elusive concept that is hard to target.} []
\newline
% https://arxiv.org/pdf/2209.00366.pdf

\noindent

Contemporary research does not provide a unified approach to measuring "goodness" of an explanation.  There are attempts to propose a set of properties a good explainability method should fulfill, but they are not aligned [2, 3]. What is more, some studies even present contradictory results, rendering objective conclusions even more challenging [4, 5]. According to Doshi-Velez [], "the field is crowded with evaluation methods, and there is no consensus on which are the “right” ones. Much less, there is not even agreement on which criteria should be evaluated". The challenge lies in the absence of ground truth for an explanation, and therefore we have no objective measure of grading the explanations []. However, through careful choice of metrics, we can reason that given audience and use-case, certain explainability method is sufficient. To avoid so-called "anecdotal evidence", we need to carefully pick our metrics, so that we ensure that explanation is:

% doshi How to Evaluate Explainability? – A Case for Three Criteria
% A Benchmark for Interpretability Methods in Deep Neural Networks

\begin{enumerate}
    \item Performant: This is the main bottleneck of current solution of Occlusion-based saliency. Without reasonable performance, we do not care how good a method is, since it cannot be utilized for practical purposes.
    \item Faithful: Ideally, we want to ensure that the explainability method only highlight the relevant parts of input.
    \item Useful: We need the explanations to be presented to domain experts in such way, that it is understandable for for them. If they do not understand the explanation, it is of little value. 
    
\end{enumerate}

\section{Computational complexity}

The main problem of the existing approach to generate explanations is the required time.

\subsection*{Time efficiency}

In order for the method to actively assist pathologist, the explanation needs to be computed fast enough to not disrupt his workflow. The main caveat of the current solution untilizing occlusion is that it takes too long. We let each method fully explain averagely-sized slide from test set. Each run is conducted 3 times, to make sure the result is not influenced by any momentarily factors. Results are presented in Table ...

\subsection*{GPU utilization}

Modern GPU's offer possibility of multiple processes performing computations on the same instance. For production deployment, this is important factor. High GPU utilization for single method can negatively affect number of parallel processes and therefore raise costs of production deployment. Since neural networks are generally expensive in terms of computational power, their usage inevitable leads to increase in carbon emissions. Since sustainability is a hot topic, it is wise to consider this aspect for future deployment and development.

\section{Quantitative evaluation}

This section covers use of several widely used metrics capturing desired traits of explainability methods.

\subsection*{Faithfulness}

Common approach to assess whether explainability method marks the relevant part of input is to perturb the image such that we remove those marked features -- similar to how occlusion estimates the importance. However, there are studies indicating that simply removing those features may not be correct, since the perturbed input can contain certain artifacts, leading to a distribution shift. This may affect the plausibility of such metrics, since we cannot tell whether the increase/drop in model's confidence is caused by distribution shift, or the features were really important [ROAR].

In paper Gallo et al., only methods suffering from potential artifact introduction were used to measure the faithfulness. Notably, Occlusion received the highest scores, aligned with observation by ... -- that such method may favor methods, which rely on the same mechanism when estimating importance on input features [].
% https://arxiv.org/pdf/2006.00442.pdf

To tackle the distribution issue, Hooker et al. [ROAR] present metric called Remove and Retrain (ROAR), which first removes the marked features, and then retrains the model on newly created dataset. While this method got widely accepted, it brings significant drawback in form of how computationaly exhaustive would this method be for our use-case. As ... shown, ROAR does not solve the problem of \emph{class information leakage} -- phenomenon, when the uniformly-valued mask itself reveals relevant class info through its shape [ROAD].

Rong et al. propose method building on foundations of ROAR, called Remove and Debias (ROAD). Instead of retraining the model, areas after removed features are imputed in such fashion that risk of class information leakage is reduced. For further reading and information theory behing imputation, refer to original paper [].

To evaluate performance of methods described in Section ..., we will iteratively remove features from the most important to the least (MoRF order). After each removal, missing features are imputed using noisy-linear imputer, to reduce class information leakage. We evaluate the metods at $10, 20, 30, 40, 50$ percents removed, as in the original paper. It is desired, that score should drop significantly as we start removing the features and the rate slows down as we approach the $50$\% border, as it signals that the important parts were removed as first. For evaluation, we used implementation from \emph{pytorch-gradcam} package.
% road

\subsection*{Robustness}
% max sensitivity

\subsection*{Localization}
% localization

\subsection*{Usefulness}

\section{Domain expert assessment}
