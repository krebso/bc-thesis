\chapter{Introduction}\label{chap:introduction}

With deep learning and neural networks in the spotlight of contemporary research, we see their applications in a wide range of areas.

However, in places where the capabilities of deep learning models could be the most helpful, deployed neural networks suffer from their ``black box'' tag and the lack of insight behind their decisions.

In such critical areas as medicine, self-driving, or law, it is essential to have an accurate proxy to shed light on the model's decision process.

This thesis aims to find methods suitable for such reasoning.
Occlusion, a faithful method tested by Gallo et al. in \cite{gallo}, is too slow to be used in real time.
Thus, we begin our search for a faster method that does not compromise the usability and faithfulness of the explanations produced.

To find such a method, we need to understand our domain and its limitations.
Therefore, we start with an overview of essential concepts and nomenclature.
We then motivate the need for such an explaining proxy and review several well-established methods, carefully selected based on previous work of the RationAI group in  \cite{gallo, bajger-grad-cam, krajnansky-grad-cam, hruska-grad-cam}.
In order to verify that the outlined methods are indeed suitable, we establish and evaluate them against tailored quantitative and qualitative benchmarks.

