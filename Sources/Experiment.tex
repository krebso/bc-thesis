\chapter{Suitability Benchmark}\label{experiment}

This chapter consists of four sections. First, we try to outline what a good explanation should fulfill. Then, we tackle the current problem of high computational resource utilization of the current solution. In the third section, we construct a quantitative benchmark, choosing suitable metrics alongside brief reasoning and evidence behind our choice. In the last section, we conduct a subjective evaluation by a domain expert using a previously established approach.

We will use implementation from popular XAI-oriented machine learning libraries, \texttt{captum} (Occlusion, Composite-LRP), \texttt{torch-cam} (CAM) and \texttt{pytorch-grad-cam} (HiResCAM, GradCAM++, ScoreCAM, AblationCAM). All three libraries are publicly available on \texttt{github} and provide convenient interfaces, which we can easily unify in our pipeline.

CAM-based methods can compute saliency maps for arbitrary convolutional pooling layers in our network.
Given the notion of hierarchical learning of representation across the network, attributing the lower layers does not make sense.
While using lower layers 

\section{What makes a good explanation?}

\emph{``Unfortunately, `explainability' is a nebulous and elusive concept that is hard to target.''} \cite{explainability-hard}
\newline

Contemporary research does not provide a unified approach to define the "goodness" of an explanation.
There are attempts to propose a set of properties a good explainability method should fulfill, but they are not aligned \cite{xai-functionality-grounded, explainability-hard, xai-meta-survey, xai-zhou-survey}.
Moreover, some studies present contradictory results, rendering objective conclusions even more challenging \cite{xai-zhou-survey}.
According to Doshi-Velez in \cite{xai-doshi}, "the field is crowded with evaluation methods, and there is no consensus on which are the “right” ones. 
Much less, there is not even agreement on which criteria should be evaluated.".
The challenge lies in the absence of ground truth for produced explanations, and therefore, we have no objective measure of grading the explanations \cite{xai-zhou-survey}.
However, by carefully choosing metrics, we can avoid so-called "anecdotal evidence" and argue that certain explainability methods are sufficient given the audience and use case \cite{xai-anecdotal-evidence}.
We believe that for an explanation to be applicable to our use-case of generating slide-level explanations, it needs to be:
\begin{enumerate}
    \item Performant: This is the main bottleneck of the current solution. We do not care how good a method is without reasonable performance since it cannot be utilized for practical purposes.
    \item Faithful: Ideally, we want to ensure that the explainability method only highlights the relevant parts of the input tile.
    \item Useful: We need the explanations to be presented to domain experts so that it is understandable for them. If they do not understand the explanation, it is of little value. 
\end{enumerate}

\section{Computational performance}

For the method to actively assist pathologists, the explanations must be computed fast enough not to disrupt his workflow.
The main problem of the existing approach using Occlusion-based saliency is to generate explanations for an entire slide in a reasonable time.
The current solution employs \texttt{python} implementation from the \texttt{captum} package.
The problem with Occlusion is two-fold; either we perform one forward pass with the occluded tile at a time, needing XXX synchronous forward passes, or we batch several occluded tiles together, drastically raising GPU memory costs.
To tackle this problem from both perspectives --- we first measure the time required to produce a single explanation and then observe how much GPU memory it needs.

\subsection*{Time efficiency}

 We let each method fully explain the averagely-sized WSI of 1499 tiles from the test set.
 Although we aim to achieve good slide-level performance, we measure the time required for individual tile-level explanations instead.
 During the generation of slide-level explanations, there are a lot of auxiliary operations to stitch tile-level saliency maps together.
 The method and its implementation are not responsible for these operations.
 We include this overhead if we measure the time required to generate slide-level saliency maps.
 Since modern machine-learning frameworks typically employ parallel processing using multiple CPU cores, this can lead to resource locks, negatively affecting a method's performance if we were looking at the required slide-level time.
 Another reason to measure the required time for tile-level explanations is resource saving.
 We want to take multiple unit measurements and see their mean and variance since momentary factors of the machine environment can influence a single data point. 
 By taking multiple data points, we are hedging ourselves against these factors.
 If we were measuring the slide-level performance, it would be notably more time-and-resource intensive, blocking resources from other students and colleagues from the RationAI group.

 We performed each run on a machine with $8$ cores, $16$GB of RAM, and a single NVIDIA A40 GPU card with $48$GB of memory.
 For Occlusion, we used a batch size of $200$ perturbed tiles, the same settings used in the production deployment.
 Other methods do not offer such an option. We repeatedly fed each method a single tile and measured the time difference between the start and end of the computation.
 After the method call, we inserted a \texttt{torch.cuda.synchronize} call to ensure all computation on GPU is finished before taking the end-of-computation timestamp.

\begin{table}
\centering
\ra{1.2}
\begin{tabular}{@{} l r r r @{}}\toprule
Method & Avg. time per tile (s) & Time for slide (s) & Expected largest slide time (m) \\ 
\midrule
CAM           & $0.04 \pm 0.04$ & $59.45$    & $2.73$ \\
GradCAM++     & $0.08 \pm 0.00$ & $131.88$   & $6.05$ \\
HiResCAM      & $0.08 \pm 0.00$ & $131.95$   & $6.06$ \\
Composite-LRP & $0.11 \pm 0.00$ & $178.829$  & $8.21$ \\
\textbf{Occlusion}     & $1.06 \pm 0.00$ & $1612.70$  & $74.07$ \\
AblationCAM   & $9.72 \pm 0.00$ & $14583.50$ & $669.82$ \\
ScoreCAM      & $9.76 \pm 0.00$ & $14629.98$ & $671.96$ \\
\bottomrule
\end{tabular}
\caption{
Time efficiency of overviewed XAI methods.
Methods are ordered from the fastest to the slowest.
Entry for Occlusion is in bold to visually delimit the methods into faster and slower than the current solution.
Unsurprisingly, methods that require a single pass to compute activation maps' importance were the fastest, while the methods perturbing either input or model internals scored slower since they require multiple forward passes to estimate the importance.
We could solve this, as in the case of Occlusion, by batching multiple perturbed inputs together, but the libraries do not offer such an option.
Even then, we would arrive at a different problem; such batching requires tremendous GPU capacity.
The third column represents the estimated time to explain the largest slide from the test split to give a current worst-case scenario of $4131$ tiles-per-slide outlook.
}
\label{tab:comp-time}
\end{table}
\todo{minutes}

\myref{Table}{tab:comp-time} shows the superiority of CAM methods, which require a single forward pass to compute the saliency maps.
Expectedly, ScoreCAM scored the worst.
This stems from the fact that for each of the $512$ activation maps of the last layer of our model, ScoreCAM runs a separate forward pass with a perturbed input tile.
The problem of multiple forward passes is tamed by batching in the case of Occlusion, where most of the overhead comes from the creation of perturbed tiles.
What surprised us was the time AblationCAM needed to compute a single explanation.
\myref{Equation}{eq:ablation-cam-importance-weight} shows that the importance weights in the case of AblationCAM are computed by perturbing models internals --- systematically zeroing out activations across layer units.
In the case of our model, computing importance weight for unit $k$ means setting the pooled activation $a^k$ to zero --- output $y^c_k$ is simply a linear combination of pooled activations, omitting the $a^k$, which we would expect to be computed reasonably fast.

Since both AblationCAM and ScoreCAM performed notably worse than Occlusion and did not bring us closer to finding a performant XAI method, we decided not to include them in further benchmarks.

\subsection*{GPU utilization}

Modern GPUs offer the possibility of simultaneous execution of multiple processes on the same card instance.
The demand a method places on GPU resources is crucial for production deployment.
High GPU utilization by one method can limit the number of processes that can run in parallel, which, in turn, may increase deployment costs due to the need for additional GPU cards.
Since neural networks are generally expensive in terms of computational power, their usage inevitably leads to increased carbon emissions.
Sustainability, nowadays a crucial concern in technology development and deployment, should be thoughtfully weighed when considering the future use and advancement of deep learning models.
We are not aware of any previous work trying to focus on GPU utilization of explainability methods, but given EU regulations on AI from \myref{Section}{sec:need-for-xai}, we believe that it should be evaluated accordingly.
Not considering those requirements could negatively affect deployed models since being unable to facilitate given restrictions may require them to be brought down.

Like the Time Efficiency metric, we let each method explain a single WSI of $1499$ tiles.
We monitor the GPU usage by running the \texttt{nvidia-smi} executable in a separate process, forwarding its output every $500$ milliseconds to a CSV file.
\myref{Table}{tab:gpu-util} presents the utilization of a single A40 GPU card, while computing the explanations.
Note that we cannot exactly distinguish which portion of the GPU memory the XAI method uses since we also store the model on GPU.
For this reason, we conducted one run without computing any explanations to serve as a baseline.
We did all of the runs using a batch size of one in an attempt not to discriminate any method.
We expect all methods to score better than Occlusion since none relies on internal batching.

The results in \myref{Table}{tab:gpu-util} confirm our expectations that none of the methods except Occlusion noticeably exhaust the GPU.
Therefore, we can run multiple processes simultaneously on a single card instance.
Moreover, all methods but Occlusion can be sufficiently run on the smallest GPU the RationAI group has at hand, having a capacity of $10$GB.
\begin{table}
\centering
\ra{1.2}
\begin{tabular}{@{} l r r @{}}\toprule
Method & Absolute GPU utilization (MB) & Relative Overhead (MB) \\ 
\midrule
Vanilla run             & $910$      & -       \\
CAM                     & $1084$     & $174$   \\
GradCAM++               & $1578$     & $668$   \\
HiResCAM                & $1578$     & $668$   \\
Composite-LRP           & $2476$     & $1566$  \\
\textbf{Occlusion}      & $36308$    & $35398$ \\
\bottomrule
\end{tabular}
\caption{
GPU utilization of overviewed XAI methods. We use mode instead of mean, as we observed that after two GPU memory utilization snapshots, the number stays the same up until the end of the benchmark. Column Absolute GPU utilization captures the raw output of \texttt{nvidia-smi} command, while column Relative Overhead corresponds to the additional consumed memory on top of the normal model's requirements. All values are reported in megabytes. All methods scored notably better than Occlusion, which we attribute mainly to not running batched forward passes. CAM achieved the lowest utilization since it only multiplies activation maps with FC layer weights. GradCAM++ and HiResCAM need to compute and retain certain gradients, which require more memory than vanilla pass. Composite-LRP runs one full backward pass equivalent and needs to retain forward pass activations to compute neuron relevance.
}
\label{tab:gpu-util}
\end{table}


\section{Quantitative evaluation}

This section covers several entrenched metrics that capture our desired traits of explainability methods.
We build on the work of \cite{gallo} by incorporating state-of-the-art metrics that address the limitations of previous approaches.
Refining previously used metrics aims to provide a more robust and reliable benchmark for evaluating our methods, aligning the quantitative assessment more closely with domain-specific boundaries and expectations.

\subsection*{Faithfulness}

An established approach to assess whether the explainability method points to the relevant part of the input is to perturb the image, such that we remove features perceived as important by the model and back-fill removed areas with certain fixed value --- similar to how occlusion estimates feature importance --- and observe how model's confidence changes.
However, the literature suggests that this perturbation approach, which involves filling the removed areas with zeros or the mean pixel value, could be flawed.
The perturbed input might include artifacts that induce a shift in the data distribution, compromising the reliability of such metrics since we cannot tell to what extent the change in the model's confidence stems from introduced artifacts compared to the initial relevance of removed features \cite{roar}.

Experiments in \cite{gallo} utilize methods prone to introducing such unintentional artifacts.
Notably, Occlusion and Deep-Taylor Decomposition \cite{xai-dtd} received the highest scores.
This is aligned with the observation by Hsieh et al. in \cite{xai-hsieh-occ-dtd} --- that such metrics may favor methods that rely on the same mechanism when estimating the importance of input features.
Since Occlusion estimates feature importance by a drop in the model's confidence by perturbing the input image with a patch --- when we later remove the region corresponding to a high attribution, the model's score will inevitably drop.
Employed metrics (Causal Deletion \cite{xai-causal-deletion} and Area Over Perturbation Curve \cite{xai-aopc}) also did not produce aligned results, confirming findings in \cite{roar}.

Hooker et al. \cite{roar} introduced an alternative metric known as Remove and Retrain (ROAR), which involves removing (zeroing out) the identified important features and then retraining the model with the modified dataset --- removing the distribution shift.
Although ROAR has gained in popularity, it has the significant downside of being computationally intensive, rendering it unacceptable for our use case.
\todo{maybe say it is shit cause we do not know to what extent the performance deterioration is thanks to bad training and removed data}
As further shown by Rong et al. in \cite{road}, ROAR does not solve the problem of the so-called \emph{class information leakage} --- phenomenon when the uniformly-valued removed region reveals relevant class info through its shape.
Rong et al. propose a method built on the foundations of ROAR, called Remove and Debias (ROAD).
Instead of retraining the model, areas after removed features are specifically imputed to reduce the risk of class information leakage \cite{road}. Refer to the original paper for mathematical intrications of information theory behind the imputation.

To evaluate the performance of our methods, ROAD iteratively removes features from the most relevant to the least (MoRF order).
After each removal, missing features are imputed using a noisy-linear imputer to reduce class information leakage \cite{road}.
For a single tile, $x$, saliency map $S$ and percentage $p$, the result of this method is a number $d$ corresponding to the drop in confidence of a model computing function $f$ when fed the imputed tile
\begin{equation}
    d = f(x) - f\bigl(\operatorname{perturb}_p(x, S)\bigr).
\end{equation}
The function $\operatorname{perturb}_p$ takes the input image $x$, saliency map $S$, and linearly imputes the areas corresponding to the top $p$ percent of most salient pixels of explanation $S$ \cite{road}.

As in one of the experiments in the original paper, we evaluate the methods at $10, 20, 30, 40$, and $50$ percent of the most salient pixels perturbed.
While other experiments in \cite{road} feature imputation up to $85$ percent, it is computationally not feasible in our setting.
Since our methods tend to cover larger areas of input tiles, imputing $85$ of the most salient pixels takes around \todo{add the final time}.
Moreover, in the production setting, the Occlusion saliency maps are thresholded to display anywhere from $55$ to $75$ of the most salient pixels.
As this thesis aims to compare the given methods against the baseline generated by Occlusion, with all these factors in mind, we decided not to extend the perturbation beyond the $50$ percent of the most salient pixels.

It is desired that the model's score should drop significantly as we start removing the features, and the rate slows down as we approach the higher percentages, signaling that the important parts were indeed removed in the beginning.
We evaluated our model using the \texttt{pytorch-grad-cam} package that provides the \texttt{NoisyLinearImputer} class.
This class is designed to handle the imputation of missing values in the same manner as the original implementation from the ROAD paper.
We did not use the original implementation from \cite{road}, as the interface did not fit into our data-processing pipeline.
\myref{Figure}{fig:road-impute} shows how the \texttt{NoisyLinearImputer} imputes removed areas.
Since this method relies on sensitivity, we will only use positive tiles from the test split of the dataset from \myref{Section}{sec:dataset}.
To see the curve of the drop in the model's confidence, refer to \myref{Figure}{fig:road-curve}.
A boxplot portraying the distribution of our results is presented in \myref{Figure}{fig:road-boxplot}.

Our findings are consistent with the study conducted by Gallo et al. \cite{gallo} --- the faithfulness metric we utilized tends to prefer techniques that identify broader, continuous regions within the input tile.
As shown in \myref{Figure}{fig:road-impute}, the tile imputed based on the explanation generated by Occlusion looks visually more distressed than the tile imputed according to the more scattered Composite-LRP method.
This  HiResCAM, which scored worst out of all CAM-based methods since it is the most conservative regarding how much of a tile-produced saliency map covers.
\todo{obrazok alebo to lepsie vysvetlit}
According to \myref{Figure}{fig:road-boxplot}, CAM-based methods produce more consistent results than Occlusion-based saliency.
Similar to $\varepsilon$-LRP in \cite{gallo}, Composite-LRP performs worse than Occlusion, rendering our effort to assign different relevance rules to produce more coherent saliency maps according to \cite{lrp} unsuccessful.
An interesting observation is that upon perturbation based on saliency maps produced by Occlusion and CAM, the models' confidence does not increase, which is not the case for the rest of the methods.
We also posit that despite the authors of GradCAM++ and HiResCAM \cite{grad-cam, hires-cam} advertising their methods as more faithful than CAM/GradCAM, our results suggest otherwise.

\todo{legendu k ciaram}
\begin{figure}
    \begin{center}
    \begin{minipage}{0.7\textwidth}
      \includegraphics[width=\textwidth]{img/road-curve.png}
    \end{minipage}
    \caption{Curve visualizing mean drop in confidence of individual methods per imputed percentages of most salient pixels. Notice that looking only at the mean renders CAM and GradCAM++ as methods achieving similar performance.}
    \label{fig:road-curve}
    \end{center}
\end{figure}

\begin{figure}
    \begin{center}
    \begin{minipage}{1\textwidth}
      \includegraphics[width=\textwidth]{img/road-boxplot.png}
    \end{minipage}
    \caption{Boxplot of results for different percentages of ROAD method. We used the percentages from \cite{road}. ROAD ranks CAM methods over Occlusion and Composite-LRP. We can see that perturbation by CAM-based method explanations also decreases the model's confidence faster than the Occlusion method --- advocating for the CAM explanations to be more faithful. Scores for CAM methods also have less variance when significant parts of salient areas are removed. Notice the minimum values and that only CAM and Occlusion do not increase models' confidence upon evaluation of perturbed tile, while gradient-based CAMs and Composite-LRP do.}
    \label{fig:road-boxplot}
    \end{center}
\end{figure}

\begin{figure}
    \begin{center}
    \begin{minipage}{0.8\textwidth}
      \includegraphics[width=\textwidth]{img/road-impute.png}
    \end{minipage}
    \caption{Upper row depicts attributions for Occlusion and Composite-LRP for the same tile. The bottom row shows a visualization of the tile imputed based on the $20$ percent of the most salient attribution pixels. While \texttt{NoisyLinearImputer} removes continuous regions based on the Occlusion attribution, the scattered Composite-LRP attribution and following imputation lead to a tile visually very similar to the original one. As a result, methods producing less-scattered explanations received higher scores, in agreement with results from \cite{gallo}. Images featuring attributions are presented in black and white to increase the visual contrast between the tile and the explanation. It is important to acknowledge that ROAD favors methods that cover larger areas of the tile --- because, at the same threshold, larger parts of the tile will be imputed. However, the visualization may be misleading, as LRP positively attributed more than $58$ percent of the displayed tile.}
    \label{fig:road-impute}
    \end{center}
\end{figure}

\subsection*{Localization}

The faithfulness metric tells us how well the explainability method attributes important locations.
In addition, we want to ensure that these locations resemble what pathologists would classify as adenocarcinoma.
To measure how well the output of a given method matches the pathologists' annotation, Gallo et al. \cite{gallo} utilized the Effective Heat Ratio (EHR) \cite{ehr} metric.
EHR relies on ground truth bounding boxes, which encapsulate objects of interest.
Because of our specific use case, we argue that EHR is not the most suitable method.
Given a bounding box, by design, EHR favors methods that cover larger annotation areas with strong saliency.
This is not necessarily desired, as the bounding box only marks the rough location of cancerous tissue --- precise pixel-level borders are difficult to follow \cite{gallo, annotation-agreement} and such precise annotation would likely vary from pathologist to pathologist \cite{annotation-agreement}.
Therefore, EHR discriminates metrics that could potentially point to cancerous markers but do not attribute healthy surrounding tissue included in the annotation.

We will use what we consider a simpler technique called Weighting Game from \cite{weighting-game}, instead.
It builds upon the established Pointing Game metric \cite{pointing-game}, which looks at whether the most salient pixel falls into the bounding box.
Unlike the Pointing Game, which gives us just binary information about the accuracy of a given method, the Weighting Game calculates the ratio of the mass of the saliency map $S$ within the bounding box to the total mass of the $S$ \cite{weighting-game}.
Compared to EHR, this does not disqualify methods that highlight smaller parts of the annotated area, and we believe it gives us a fair measure of how the explanation holds compared to the pathologist's annotation.
Given a tile $x$, annotation in the form of binary mask depicting the annotated region $A$ and saliency map $S$, the ratio $r$ is calculated as
\begin{equation}\label{eq:wg}
    r = \frac{\operatorname{\text{mass}}(S \odot A)}{\operatorname{\text{mass}}(S)}.
\end{equation}
In the Weighting Game paper \cite{weighting-game}, authors first blur the mask a little by convolving it with a $3 \times 3$ filter to counter possible imprecision given a pixel-level ground truth bounding boxes.
We decided to omit this explicit blurring step since, given our domain, the annotations are implicitly not ground truth for exact borders of cancerous tissue, and such imprecisions are already counted for given the more rough polygonal shape.

It is important to understand the role of this metric in our benchmark.
Post-hoc XAI methods serve as a proxy between a model and its user.
The model may have its flaws and hidden biases, and therefore, we cannot disregard a method solely on a bad Weighting Game score.
Such a method may still reveal important information and help to facilitate such flaws and biases.
However, our model is believed to have an excellent performance, rigorously verified with a domain expert in \cite{gallo}.
Therefore, we see this metric as a bridge between the quantitative faithfulness metric and qualitative but inherently subjective domain expert assessment.

\myref{Figure}{fig:weighting-game-boxplot} depicts rations distribution per positive tiles in the test part of our dataset.
Since we threshold Occlusion saliency maps when looking at the WSIs in the browser, we evaluate across different percentages of the most salient pixels kept.
Gradient-based CAMs outperform Occlusion across all thresholds, advocating for the positive influence of gradient on localization capabilities.
CAM performs similarly, slowly catching up as we keep only the most salient attributions.
Notice that the score for Composite-LRP improves very slowly.
This likely means that Composite-LRP tends to strongly attribute features that do not fall within the annotated region, and these attributions are preserved even upon keeping only the very most salient regions.


\begin{figure}
    \begin{center}
    \begin{minipage}{1\textwidth}
      \includegraphics[width=\textwidth]{img/weighting-game-boxplot.png}
    \end{minipage}
    \caption{Boxplot of the Weighting Game rations across different percentages of the most salient pixels kept. Notice the very good localization capabilities of gradient-based CAMs from the very beginning. This is not surprising in the case of HiResCAM since it only averages over the strongest activations, rendering the saliency map more conservative regarding how much of the tile they cover. However, we expected CAM to have similar performance to GradCAM++. T}
    \label{fig:weighting-game-boxplot}
    \end{center}
\end{figure}

\subsection*{Usefulness}

The usefulness of an explanation in the context of deep learning models is inherently subjective, as it can vary significantly depending on the individual's perspective and context --- the audience's background, expertise, and purpose of use all play critical roles in determining the perceived utility of an explanation \cite{xai-doshi} --- something hard to capture by a quantitative metric.

Fortunately, we know that a solution based on Occlusion produces semantically correct saliency maps aligned with features recognized by pathologists \cite{gallo}.
Thus, we will employ the Weighting Game metric to assess how well candidate methods' saliency maps resemble the Occlusion ones.
Therefore, to create an Occlusion-based annotation, we convert the Occlusion saliency map to a binary mask, where positive values get the value of $1$.
In practice, during their analysis, the pathologist adjusts the saliency threshold of the displayed explanations to fall between approximately $55$ to $75$.
Since Occlusion-based annotations cannot be considered a ground truth for relevant morphological features, we will give the candidate methods the benefit of the doubt, creating the annotations at the lower bound of the threshold, $55$ percent.
Then, we use this thresholded binary saliency map as annotation $A$ in \myref{Equation}{eq:wg}.

We do not perceive this metric as adding to candidate methods' faithfulness or localization capabilities.
Instead, its purpose is twofold.
First, it guides our selection of which explainability methods to present to a pathologist, allowing us to prioritize explanations that align most closely with the established Occlusion baseline.
Second, it enables us to evaluate how the pathologist subjectively perceives candidate explainability techniques relative to the established understanding and acceptance of Occlusion.

\myref{Figure}{fig:occ-weighting-game-boxplot} depicts the distribution of Weighting Game ratios across different thresholds for candidate saliency maps.
HiResCAM agrees well with Occlusion-based annotation.
This aligns with our a priori expectation from \myref{Subsection}{sub:hirescam} that Occlusion will attribute mostly regions corresponding to the highest activation maps values since only those are taken into account because of GMP.
Despite GradCAM++ and CAM producing visually similar results, GradCAM++ saliency maps tend to hit better areas attributed by Occlusion.
However, CAM starts to catch up towards the higher thresholds for kept pixels.
Composite-LRP consistently performs across all thresholds, supporting the observation that it attributes regions not considered important by other methods.


\begin{figure}
    \begin{center}
    \begin{minipage}{1\textwidth}
      \includegraphics[width=\textwidth]{img/occlusion-weighting-game-boxplot.png}
    \end{minipage}
    \caption{Boxplot of Weighting Game ratios, when we take $55$ percents of the most salient pixels from Occlusion saliency map as the bounding box. Notice that HiResCAM achieves high ratios very consistently. We attribute the high score to the conservativeness of areas covered by its saliency maps and our expectation from \myref{Subsection}{sub:hirescam}. Despite the similar performance of CAM and GradCAM++ in both faithfulness and localization metrics, GradCAM++ tends to better resemble the Occlusion-based annotation, up until taking only the very most salient pixels from respective saliency maps.}
    \label{fig:occ-weighting-game-boxplot}
    \end{center}
\end{figure}

\section{Domain expert assessment}

This section presents a qualitative evaluation of produced explanations by domain experts.
In \cite{gallo}, Gallo et al. first generated saliency maps for all test slides.
Then, they sampled a subset of $461$ regions, where Occlusion suggested an area important for the model when deciding whether to be pro or against the presence of cancer.
Originally, we wanted to present candidate saliency maps for all the $461$ original regions.
However, this would implicitly handicap methods, which focus on different, albeit still relevant, parts of the WSI. 
Another option was to produce saliency maps of candidate methods for the test set and try to sample new areas of interest. However, we could not use the original approach since it also relied on negative attributions, something we do not have for HiResCAM and GradCAM++.
Moreover, such random sampling could also be unfair, as the distribution of different morphological features would likely vary across different samples.
A rigorous and thorough review of detected cancerous markers in hundreds of samples also requires a non-trivial time investment from a domain expert.

With all that in mind, we reviewed the results of our quantitative metrics and decided to reframe the assessment.
We leverage that our domain expert already has prior experience with results based on Occlusion.
Therefore, we generate saliency maps for candidate methods for all test WSIs.
Since the pathologist's time is not to be wasted, we narrowed down the presented methods based on their quantitative results.
We decided to include explanations of two methods, CAM and HiResCAM.
CAM has comparable quantitative results to GradCAM++ while being faster.
Unlike GradCAM++, saliency maps from CAM also contain negative regions, pointing to locations the model perceives as against cancer --- same as Occlusion.
HiResCAM scored notably well when quantitatively comparing marked salient regions to the Occlusion ones.
In this setting, the target audience is an experienced pathologist, MUDr. Rudolf Nenutil, CSc.
He has a long track record of working with the national group, and his expertise was used for evaluation in \cite{gallo}.
To subjectively assess the usefulness of the saliency maps, we give him the following assignment:

\emph{
In the attached URI, you will find a report with all test slides and saliency maps generated by our candidate methods.
Open them in the WSI browser and subjectively assess whether they could be used instead of the Occlusion saliency maps.
As for Occlusion, try to find a suitable threshold such that the saliency points to the relevant morphological features.
Assess whether those methods point to features and patterns you understand.
Saliency maps of CAM contain green and red regions, with the same meaning as in the case of Occlusion --- green areas denote pro-cancerous features, while red areas denote non-cancerous tissue.
Saliency maps for HiresCAM are in yellow, depicting only the pro-cancerous areas.
Please document your findings, noting where the saliency maps align well or poorly with your understanding of the tissue morphology.
}

